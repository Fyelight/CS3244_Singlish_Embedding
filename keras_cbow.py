# -*- coding: utf-8 -*-
"""keras cbow

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1bcGEl24veoNgjlo-wV9cv8O1G7W8G5NA
"""

import string

import numpy as np
import pandas as pd
import tensorflow as tf
from keras import backend as K
from keras import optimizers
from keras.layers import Embedding, Lambda, Dense
from keras.models import Sequential

sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))
print(sess)

df = pd.read_csv('COMBINED dataset.csv', index_col=None)
NUM_SENTENCES = len(df['0'])
sentences = df['0'].astype(str)

translation = string.maketrans(string.ascii_letters, string.ascii_letters)

f = lambda x: x.translate(translation)
sentences = sentences.apply(f)


def raw_words(corpus):
    stop_words = ['is', 'a', 'will', 'be']
    words = []
    for sentence in corpus:
        sentence = sentence.split()
        words.extend(sentence)
    words = set(words)
    for stop in stop_words:
        if stop in words:
            words.remove(stop)
    words = list(words)
    wordToint = {}
    for (index, word) in enumerate(words):
        wordToint[word] = index

    return wordToint

def remove_stop_words(corpus):
    stop_words = ['is', 'a', 'will', 'be']
    clean_data = []
    for sentence in corpus:
        sen = sentence.split()
        for word in stop_words:
            while word in sen:
                sen.remove(word)

        clean_data.append(sen)
    return clean_data


def generate_data(sentences):
    data = []
    for sentence in sentences:
        for i in range(2, len(sentence) - 2):
            try:
                context = [wordToint[sentence[i - 2]], wordToint[sentence[i - 1]], wordToint[sentence[i + 1]],
                           wordToint[sentence[i + 2]]]
                target = wordToint[sentence[i]]
                data.append((context, target))
            except:
                print(sentence)

    return data

wordToint = raw_words(sentences)
NUM_OF_WORDS = len(wordToint)

VOCAB_SIZE = len(wordToint)
dim_embedddings = 20

sentences = remove_stop_words(sentences)
data = generate_data(sentences)
data = np.array(data)

x = [context for (context, target) in data]
y = [target for (context, target) in data]
x = np.array(x)
y = np.array(y)


model = Sequential()
model.add(Embedding(VOCAB_SIZE, dim_embedddings, input_shape=(4,)))
model.add(Lambda(lambda x: K.sum(x, axis=1), output_shape=(dim_embedddings,)))
model.add(Dense(VOCAB_SIZE, activation='softmax'))

opt = optimizers.Adam(lr=0.1)

model.compile(optimizer=opt,
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

model.summary()

model.fit(x, y, epochs=500, batch_size=1024)

f = open('vectors.txt', 'w')
f.write('{} {}\n'.format(NUM_OF_WORDS, dim_embedddings))
vectors = model.get_weights()[0]
for word, i in wordToint.items():
    f.write('{} {}\n'.format(word, ' '.join(map(str, list(vectors[i, :])))))
f.close()
